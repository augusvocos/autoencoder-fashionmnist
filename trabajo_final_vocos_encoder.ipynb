{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "I8N3D_nU1_oT"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import datetime\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.linalg as linalg\n",
    "import sklearn as skl\n",
    "import pandas as pd\n",
    "import json\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, random_split\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms import ToTensor, Lambda, Compose\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7659,
     "status": "ok",
     "timestamp": 1736504412170,
     "user": {
      "displayName": "MATÍAS GINO PRANZONI",
      "userId": "09458630644847038155"
     },
     "user_tz": 180
    },
    "id": "NUoQ9bnwaZ7O",
    "outputId": "a4a751ea-3917-436d-87b7-1eea3ca9e102"
   },
   "outputs": [],
   "source": [
    "# Descargo y preparo los dataset para el autoencoder\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "train_set_orig = datasets.FashionMNIST('MNIST_data/', download = True, train = True,  transform = transform)\n",
    "valid_set_orig = datasets.FashionMNIST('MNIST_data/', download = True, train = False, transform = transform)\n",
    "\n",
    "class CustomDataSet(Dataset):\n",
    "    def __init__(self,dataset):\n",
    "        self.dataset = dataset\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    def __getitem__(self,i):\n",
    "        image, label = self.dataset[i]\n",
    "        input = image\n",
    "        output = image\n",
    "        return input,output\n",
    "\n",
    "train_set = CustomDataSet(train_set_orig)\n",
    "valid_set = CustomDataSet(valid_set_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "qSJqCozXCEq1"
   },
   "outputs": [],
   "source": [
    "# Calcula la dimensión de la salida de una convolución 2D\n",
    "# Conv2D de PyTorch es así: torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)    \n",
    "\n",
    "def convt2d_z_out(z_in, kernel_size,output_padding, stride = 1, padding = 0, dilation = 1):\n",
    "    z_out=(z_in-1)*stride-2*padding+dilation*(kernel_size-1)+output_padding\n",
    "    return z_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "FCZX-wfTUgBP"
   },
   "outputs": [],
   "source": [
    "class Autoencoder_Conv(nn.Module):\n",
    "    def __init__(self, n=28*28,p=0.2):\n",
    "        super().__init__()             \n",
    "        self.n = n\n",
    "        self.p = p\n",
    "\n",
    "        #Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            \n",
    "            nn.Conv2d(in_channels=1,out_channels=16,kernel_size=3), #(1, 28, 28) -> (16, 26, 26)\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.p),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2), # (16, 26, 26) -> (16, 13, 13)\n",
    "            \n",
    "            nn.Conv2d(in_channels=16,out_channels=32,kernel_size=3), # (16, 13, 13) a (32, 11, 11)\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.p),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2), #(32, 11, 11) -> (32, 5, 5)\n",
    "\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32*5*5,self.n), #(32, 5, 5) -> 32*5*5\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.p) # 32*5*5 -> n\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "\n",
    "            nn.Linear(in_features=self.n,out_features=32*5*5), # n -> 32*5*5\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.p),\n",
    "            nn.Unflatten(1,(32,5,5)), # 32*5*5 -> (32, 5, 5)\n",
    "\n",
    "            nn.ConvTranspose2d(in_channels=32,out_channels=16,kernel_size=5,stride=2,padding=2,output_padding=0,dilation=2), # (32, 5, 5) -> (16, 13, 13)\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.p),\n",
    "\n",
    "            nn.ConvTranspose2d(in_channels=16,out_channels=1,kernel_size=6,stride=2,padding=1,output_padding=0,dilation=1), # (16, 13, 13) -> (1, 28, 28)\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoencoderConv2(nn.Module):\n",
    "    def __init__(self, n=128, p=0.2):\n",
    "        super().__init__()\n",
    "        self.n = n\n",
    "        self.p = p\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            # Capa 1: de (1,28,28) a (32,28,28)\n",
    "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # (32,28,28) -> (32,14,14)\n",
    "            nn.Dropout(p),\n",
    "            \n",
    "            # Capa 2: de (32,14,14) a (64,14,14)\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # (64,14,14) -> (64,7,7)\n",
    "            nn.Dropout(p),\n",
    "            \n",
    "            nn.Flatten(),  # (64,7,7) -> 3136\n",
    "            nn.Linear(3136, n),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p)\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(n, 3136),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p),\n",
    "            nn.Unflatten(1, (64, 7, 7)),\n",
    "            \n",
    "            # Capa 1: de (64,7,7) a (32,14,14)\n",
    "            nn.ConvTranspose2d(in_channels=64, out_channels=32, kernel_size=3, stride=2, \n",
    "                               padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p),\n",
    "            \n",
    "            # Capa 2: de (32,14,14) a (1,28,28)\n",
    "            nn.ConvTranspose2d(in_channels=32, out_channels=1, kernel_size=3, stride=2, \n",
    "                               padding=1, output_padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "9Umd-TsuGS89"
   },
   "outputs": [],
   "source": [
    "def batch(x):\n",
    "  return x.to(device).unsqueeze(0) # (28,28) --> (1,28,28)\n",
    "\n",
    "def unbatch(x):\n",
    "  return x.squeeze().detach().cpu().numpy() # (28,28) --> (1,28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1184,
     "status": "ok",
     "timestamp": 1736504778572,
     "user": {
      "displayName": "MATÍAS GINO PRANZONI",
      "userId": "09458630644847038155"
     },
     "user_tz": 180
    },
    "id": "CVa7IEL7Gmc4",
    "outputId": "29e4b86e-16ce-4e57-b638-afbe3a257a66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n",
      "(28, 28)\n",
      "torch.Size([1, 1, 28, 28])\n",
      "(28, 28)\n"
     ]
    }
   ],
   "source": [
    "# Verifico que el modelo sea coherente en input size y output size\n",
    "model = Autoencoder_Conv()\n",
    "image,_  = train_set[1]\n",
    "image = image.to(device)\n",
    "pred = model(batch(image))\n",
    "print(image.size())\n",
    "print(unbatch(image).shape)\n",
    "print(pred.size())\n",
    "print(unbatch(pred).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "hyuXv-0x29Xw"
   },
   "outputs": [],
   "source": [
    "# Función de entrenamiento\n",
    "\n",
    "def train_loop(dataLoader,model,loss_fn,optimizer,verbose=True):\n",
    "\n",
    "  model.train()\n",
    "\n",
    "  num_samples = len(dataLoader.dataset)\n",
    "  num_batches = len(dataLoader)\n",
    "  sum_batch_avg_loss = 0\n",
    "  num_processed_samples = 0\n",
    "  \n",
    "  \n",
    "  for batch, (X,y) in enumerate(dataLoader):\n",
    "\n",
    "    batch_size = len(X)\n",
    "    num_processed_samples += batch_size\n",
    "\n",
    "    # Calculo la prediccion del modelo y el error\n",
    "    pred = model(X)\n",
    "    loss = loss_fn(pred,y)\n",
    "\n",
    "    # Anulo el gradiente, hago backpropagation para calcular los gradientes de nuevo, y actualizo los pesos\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Registro el progreso\n",
    "    batch_avg_loss = loss.item()\n",
    "    sum_batch_avg_loss += batch_avg_loss\n",
    "    if batch % (num_batches/10) == 0 and verbose:\n",
    "      print(f\"Número de batch: {batch:>5d}, Pérdida promedio del batch: {batch_avg_loss:>7f}, Muestras procesadas:[{num_processed_samples:>5d}/{num_samples:>5d}]\")\n",
    "      \n",
    "  avg_loss = sum_batch_avg_loss/num_batches\n",
    "  return avg_loss\n",
    "\n",
    "\n",
    "# Funcion de validacion\n",
    "\n",
    "def eval_loop(dataLoader,model,loss_fn):\n",
    "\n",
    "  model.eval()\n",
    "\n",
    "  num_samples = len(dataLoader.dataset)\n",
    "  num_batches = len(dataLoader)\n",
    "  sum_batch_avg_loss = 0\n",
    "  num_processed_samples = 0\n",
    "\n",
    "  with torch.inference_mode():\n",
    "\n",
    "    for X,y in dataLoader:\n",
    "\n",
    "      batch_size = len(X)\n",
    "      num_processed_samples += batch_size\n",
    "\n",
    "      # Calculo la prediccion del modelo y el error\n",
    "      pred = model(X)\n",
    "      loss = loss_fn(pred,y)\n",
    "\n",
    "      # Registro el progreso\n",
    "      batch_avg_loss = loss.item()\n",
    "      sum_batch_avg_loss += batch_avg_loss\n",
    "  \n",
    "  avg_loss = sum_batch_avg_loss/num_batches\n",
    "  return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 35211,
     "status": "ok",
     "timestamp": 1736524492938,
     "user": {
      "displayName": "Gino Pranzoni",
      "userId": "04327070207206216680"
     },
     "user_tz": 180
    },
    "id": "5K_Es9Yo7E-m",
    "outputId": "4f457c4a-9db3-44a8-8ac1-2a564831e22f"
   },
   "outputs": [],
   "source": [
    "# Entrenamiento Modelo 1: n = 256; p = 0.2; ADAM; LR = 1e-3\n",
    "\n",
    "n = 256\n",
    "p = 0.2\n",
    "autoencoder_conv_1 = Autoencoder_Conv(n=n, p=p)\n",
    "\n",
    "model1 = autoencoder_conv_1\n",
    "learning_rate = 1e-3\n",
    "\n",
    "optimizer = torch.optim.Adam(model1.parameters(), lr=learning_rate,eps=1e-08,weight_decay=0,amsgrad=False)\n",
    "\n",
    "batch_size = 100\n",
    "train_loader = DataLoader (train_set,batch_size=batch_size,shuffle=True)\n",
    "valid_loader = DataLoader (valid_set,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "list_avg_train_loss_incorrecta_1 = []\n",
    "list_avg_train_loss_1 = []\n",
    "list_avg_valid_loss_1 = []\n",
    "\n",
    "patience = 10  # Número de épocas sin mejora antes de detenerse\n",
    "min_delta = 0.001  # Mínima mejora en la pérdida de validación para considerarla una mejora\n",
    "\n",
    "best_valid_loss = float('inf')  # Inicializamos con un valor infinito\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "  avg_train_loss_incorrecta = train_loop(train_loader, model1, loss_fn, optimizer)\n",
    "  avg_train_loss = eval_loop(train_loader, model1, loss_fn)\n",
    "  avg_valid_loss = eval_loop(valid_loader, model1, loss_fn)\n",
    "  list_avg_train_loss_incorrecta_1.append(avg_train_loss_incorrecta)\n",
    "  list_avg_train_loss_1.append(avg_train_loss)\n",
    "  list_avg_valid_loss_1.append(avg_valid_loss)\n",
    "  print(\"Pérdida promedio incorrecta de entrenamiento:\",avg_train_loss_incorrecta)\n",
    "  print(\"Pérdida promedio de entrenamiento:\",avg_train_loss)\n",
    "  print(\"Pérdida promedio de validación:\",avg_valid_loss)\n",
    "\n",
    "  # Early stopping check \n",
    "  if avg_valid_loss < best_valid_loss - min_delta:\n",
    "        best_valid_loss = avg_valid_loss\n",
    "        epochs_without_improvement = 0\n",
    "  else:\n",
    "        epochs_without_improvement += 1\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"Early stopping en la época {epoch + 1}\")\n",
    "            break  # Salimos del bucle si no hay mejora\n",
    "\n",
    "print(\"Done!\")\n",
    "\n",
    "results1 = {\n",
    "    'ECM incorrectos sobre conjunto de validación': list_avg_train_loss_incorrecta_1,\n",
    "    'ECM sobre conjunto de entrenamiento n=256 p=0.2': list_avg_train_loss_1,\n",
    "    'ECM sobre conjunto de validación n=256 p=0.2': list_avg_valid_loss_1,\n",
    "    'model1_state_dict': model1.state_dict(),  # Guarda los pesos del modelo\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento Modelo 2: n = 128; p = 0.2; ADAM; LR = 1e-3\n",
    "\n",
    "n = 128\n",
    "p = 0.2\n",
    "autoencoder_conv_2 = Autoencoder_Conv(n=n, p=p)\n",
    "\n",
    "model2 = autoencoder_conv_2\n",
    "learning_rate = 1e-3\n",
    "\n",
    "optimizer = torch.optim.Adam(model2.parameters(), lr=learning_rate,eps=1e-08,weight_decay=0,amsgrad=False)\n",
    "\n",
    "batch_size = 100\n",
    "train_loader = DataLoader (train_set,batch_size=batch_size,shuffle=True)\n",
    "valid_loader = DataLoader (valid_set,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "list_avg_train_loss_incorrecta_2 = []\n",
    "list_avg_train_loss_2 = []\n",
    "list_avg_valid_loss_2 = []\n",
    "\n",
    "patience = 10  # Número de épocas sin mejora antes de detenerse\n",
    "min_delta = 0.001  # Mínima mejora en la pérdida de validación para considerarla una mejora\n",
    "\n",
    "best_valid_loss = float('inf')  # Inicializamos con un valor infinito\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "  avg_train_loss_incorrecta = train_loop(train_loader, model2, loss_fn, optimizer)\n",
    "  avg_train_loss = eval_loop(train_loader, model2, loss_fn)\n",
    "  avg_valid_loss = eval_loop(valid_loader, model2, loss_fn)\n",
    "  list_avg_train_loss_incorrecta_2.append(avg_train_loss_incorrecta)\n",
    "  list_avg_train_loss_2.append(avg_train_loss)\n",
    "  list_avg_valid_loss_2.append(avg_valid_loss)\n",
    "  print(\"Pérdida promedio incorrecta de entrenamiento:\",avg_train_loss_incorrecta)\n",
    "  print(\"Pérdida promedio de entrenamiento:\",avg_train_loss)\n",
    "  print(\"Pérdida promedio de validación:\",avg_valid_loss)\n",
    "\n",
    "  # Early stopping check \n",
    "  if avg_valid_loss < best_valid_loss - min_delta:\n",
    "        best_valid_loss = avg_valid_loss\n",
    "        epochs_without_improvement = 0\n",
    "  else:\n",
    "        epochs_without_improvement += 1\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"Early stopping en la época {epoch + 1}\")\n",
    "            break  # Salimos del bucle si no hay mejora\n",
    "\n",
    "print(\"Done!\")\n",
    "\n",
    "results2 = {\n",
    "    'ECM incorrectos sobre conjunto de validación': list_avg_train_loss_incorrecta_2,\n",
    "    'ECM sobre conjunto de entrenamiento n=128 p=0.2': list_avg_train_loss_2,\n",
    "    'ECM sobre conjunto de validación n=128 p=0.2': list_avg_valid_loss_2,\n",
    "    'model2_state_dict': model2.state_dict(),  # Guarda los pesos del modelo\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento Modelo 3: n = 64; p = 0.2; ADAM; LR = 1e-3\n",
    "\n",
    "n = 64\n",
    "p = 0.2\n",
    "autoencoder_conv_3 = Autoencoder_Conv(n=n, p=p)\n",
    "\n",
    "model3 = autoencoder_conv_3\n",
    "learning_rate = 1e-3\n",
    "\n",
    "optimizer = torch.optim.Adam(model3.parameters(), lr=learning_rate,eps=1e-08,weight_decay=0,amsgrad=False)\n",
    "\n",
    "batch_size = 100\n",
    "train_loader = DataLoader (train_set,batch_size=batch_size,shuffle=True)\n",
    "valid_loader = DataLoader (valid_set,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "list_avg_train_loss_incorrecta_3 = []\n",
    "list_avg_train_loss_3 = []\n",
    "list_avg_valid_loss_3 = []\n",
    "\n",
    "patience = 10  # Número de épocas sin mejora antes de detenerse\n",
    "min_delta = 0.001  # Mínima mejora en la pérdida de validación para considerarla una mejora\n",
    "\n",
    "best_valid_loss = float('inf')  # Inicializamos con un valor infinito\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "  avg_train_loss_incorrecta = train_loop(train_loader, model3, loss_fn, optimizer)\n",
    "  avg_train_loss = eval_loop(train_loader, model3, loss_fn)\n",
    "  avg_valid_loss = eval_loop(valid_loader, model3, loss_fn)\n",
    "  list_avg_train_loss_incorrecta_3.append(avg_train_loss_incorrecta)\n",
    "  list_avg_train_loss_3.append(avg_train_loss)\n",
    "  list_avg_valid_loss_3.append(avg_valid_loss)\n",
    "  print(\"Pérdida promedio incorrecta de entrenamiento:\",avg_train_loss_incorrecta)\n",
    "  print(\"Pérdida promedio de entrenamiento:\",avg_train_loss)\n",
    "  print(\"Pérdida promedio de validación:\",avg_valid_loss)\n",
    "\n",
    "  # Early stopping check \n",
    "  if avg_valid_loss < best_valid_loss - min_delta:\n",
    "        best_valid_loss = avg_valid_loss\n",
    "        epochs_without_improvement = 0\n",
    "  else:\n",
    "        epochs_without_improvement += 1\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"Early stopping en la época {epoch + 1}\")\n",
    "            break  # Salimos del bucle si no hay mejora\n",
    "\n",
    "print(\"Done!\")\n",
    "\n",
    "results3 = {\n",
    "    'ECM incorrectos sobre conjunto de validación': list_avg_train_loss_incorrecta_3,\n",
    "    'ECM sobre conjunto de entrenamiento n=64 p=0.2': list_avg_train_loss_3,\n",
    "    'ECM sobre conjunto de validación n=64 p=0.2': list_avg_valid_loss_3,\n",
    "    'model3_state_dict': model3.state_dict(),  # Guarda los pesos del modelo\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A partir de Modelo 1, que es el que mejor performea, se busca el mejor modelo variando la probabilidad de dropout\n",
    "\n",
    "# Entrenamiento Modelo 4: n = 256; p = 0.1; ADAM; LR = 1e-3\n",
    "\n",
    "n = 256\n",
    "p = 0.1\n",
    "autoencoder_conv_4 = Autoencoder_Conv(n=n, p=p)\n",
    "\n",
    "model4 = autoencoder_conv_4\n",
    "learning_rate = 1e-3\n",
    "\n",
    "optimizer = torch.optim.Adam(model4.parameters(), lr=learning_rate,eps=1e-08,weight_decay=0,amsgrad=False)\n",
    "\n",
    "batch_size = 100\n",
    "train_loader = DataLoader (train_set,batch_size=batch_size,shuffle=True)\n",
    "valid_loader = DataLoader (valid_set,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "list_avg_train_loss_incorrecta_4 = []\n",
    "list_avg_train_loss_4 = []\n",
    "list_avg_valid_loss_4 = []\n",
    "\n",
    "patience = 10  # Número de épocas sin mejora antes de detenerse\n",
    "min_delta = 0.001  # Mínima mejora en la pérdida de validación para considerarla una mejora\n",
    "\n",
    "best_valid_loss = float('inf')  # Inicializamos con un valor infinito\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "  avg_train_loss_incorrecta = train_loop(train_loader, model4, loss_fn, optimizer)\n",
    "  avg_train_loss = eval_loop(train_loader, model4, loss_fn)\n",
    "  avg_valid_loss = eval_loop(valid_loader, model4, loss_fn)\n",
    "  list_avg_train_loss_incorrecta_4.append(avg_train_loss_incorrecta)\n",
    "  list_avg_train_loss_4.append(avg_train_loss)\n",
    "  list_avg_valid_loss_4.append(avg_valid_loss)\n",
    "  print(\"Pérdida promedio incorrecta de entrenamiento:\",avg_train_loss_incorrecta)\n",
    "  print(\"Pérdida promedio de entrenamiento:\",avg_train_loss)\n",
    "  print(\"Pérdida promedio de validación:\",avg_valid_loss)\n",
    "\n",
    "  # Early stopping check \n",
    "  if avg_valid_loss < best_valid_loss - min_delta:\n",
    "        best_valid_loss = avg_valid_loss\n",
    "        epochs_without_improvement = 0\n",
    "  else:\n",
    "        epochs_without_improvement += 1\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"Early stopping en la época {epoch + 1}\")\n",
    "            break  # Salimos del bucle si no hay mejora\n",
    "\n",
    "print(\"Done!\")\n",
    "\n",
    "results4 = {\n",
    "    'ECM incorrectos sobre conjunto de validación': list_avg_train_loss_incorrecta_4,\n",
    "    'ECM sobre conjunto de entrenamiento n=256 p=0.1': list_avg_train_loss_4,\n",
    "    'ECM sobre conjunto de validación n=256 p=0.1': list_avg_valid_loss_4,\n",
    "    'model4_state_dict': model4.state_dict(),  # Guarda los pesos del modelo\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento Modelo 5: n = 256; p = 0.1; ADAM; LR = 1e-2\n",
    "\n",
    "n = 256\n",
    "p = 0.1\n",
    "autoencoder_conv_5 = Autoencoder_Conv(n=n, p=p)\n",
    "\n",
    "model5 = autoencoder_conv_5\n",
    "learning_rate = 1e-2\n",
    "\n",
    "optimizer = torch.optim.Adam(model5.parameters(), lr=learning_rate,eps=1e-08,weight_decay=0,amsgrad=False)\n",
    "\n",
    "batch_size = 100\n",
    "train_loader = DataLoader (train_set,batch_size=batch_size,shuffle=True)\n",
    "valid_loader = DataLoader (valid_set,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "list_avg_train_loss_incorrecta_5 = []\n",
    "list_avg_train_loss_5 = []\n",
    "list_avg_valid_loss_5 = []\n",
    "\n",
    "patience = 10  # Número de épocas sin mejora antes de detenerse\n",
    "min_delta = 0.001  # Mínima mejora en la pérdida de validación para considerarla una mejora\n",
    "\n",
    "best_valid_loss = float('inf')  # Inicializamos con un valor infinito\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "  avg_train_loss_incorrecta = train_loop(train_loader, model5, loss_fn, optimizer)\n",
    "  avg_train_loss = eval_loop(train_loader, model5, loss_fn)\n",
    "  avg_valid_loss = eval_loop(valid_loader, model5, loss_fn)\n",
    "  list_avg_train_loss_incorrecta_5.append(avg_train_loss_incorrecta)\n",
    "  list_avg_train_loss_5.append(avg_train_loss)\n",
    "  list_avg_valid_loss_5.append(avg_valid_loss)\n",
    "  print(\"Pérdida promedio incorrecta de entrenamiento:\",avg_train_loss_incorrecta)\n",
    "  print(\"Pérdida promedio de entrenamiento:\",avg_train_loss)\n",
    "  print(\"Pérdida promedio de validación:\",avg_valid_loss)\n",
    "\n",
    "  # Early stopping check \n",
    "  if avg_valid_loss < best_valid_loss - min_delta:\n",
    "        best_valid_loss = avg_valid_loss\n",
    "        epochs_without_improvement = 0\n",
    "  else:\n",
    "        epochs_without_improvement += 1\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"Early stopping en la época {epoch + 1}\")\n",
    "            break  # Salimos del bucle si no hay mejora\n",
    "\n",
    "print(\"Done!\")\n",
    "\n",
    "results5 = {\n",
    "    'ECM incorrectos sobre conjunto de validación': list_avg_train_loss_incorrecta_5,\n",
    "    'ECM sobre conjunto de entrenamiento n=256 p=0.1 lr=1e-2': list_avg_train_loss_5,\n",
    "    'ECM sobre conjunto de validación n=256 p=0.1 lr=1e-2': list_avg_valid_loss_5,\n",
    "    'model5_state_dict': model5.state_dict(),  # Guarda los pesos del modelo\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A partir de Modelo 4, que es el que mejor performea, se busca el mejor modelo variando el tamaño del los batches\n",
    "\n",
    "# Entrenamiento Modelo 6: n = 256; p = 0.1; ADAM; LR = 1e-3; batch_size = 200\n",
    "\n",
    "n = 256\n",
    "p = 0.1\n",
    "autoencoder_conv_6 = Autoencoder_Conv(n=n, p=p)\n",
    "\n",
    "model6 = autoencoder_conv_6\n",
    "learning_rate = 1e-3\n",
    "\n",
    "optimizer = torch.optim.Adam(model6.parameters(), lr=learning_rate,eps=1e-08,weight_decay=0,amsgrad=False)\n",
    "\n",
    "batch_size = 200\n",
    "train_loader = DataLoader (train_set,batch_size=batch_size,shuffle=True)\n",
    "valid_loader = DataLoader (valid_set,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "list_avg_train_loss_incorrecta_6 = []\n",
    "list_avg_train_loss_6 = []\n",
    "list_avg_valid_loss_6 = []\n",
    "\n",
    "patience = 10  # Número de épocas sin mejora antes de detenerse\n",
    "min_delta = 0.001  # Mínima mejora en la pérdida de validación para considerarla una mejora\n",
    "\n",
    "best_valid_loss = float('inf')  # Inicializamos con un valor infinito\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "  avg_train_loss_incorrecta = train_loop(train_loader, model6, loss_fn, optimizer)\n",
    "  avg_train_loss = eval_loop(train_loader, model6, loss_fn)\n",
    "  avg_valid_loss = eval_loop(valid_loader, model6, loss_fn)\n",
    "  list_avg_train_loss_incorrecta_6.append(avg_train_loss_incorrecta)\n",
    "  list_avg_train_loss_6.append(avg_train_loss)\n",
    "  list_avg_valid_loss_6.append(avg_valid_loss)\n",
    "  print(\"Pérdida promedio incorrecta de entrenamiento:\",avg_train_loss_incorrecta)\n",
    "  print(\"Pérdida promedio de entrenamiento:\",avg_train_loss)\n",
    "  print(\"Pérdida promedio de validación:\",avg_valid_loss)\n",
    "\n",
    "  # Early stopping check \n",
    "  if avg_valid_loss < best_valid_loss - min_delta:\n",
    "        best_valid_loss = avg_valid_loss\n",
    "        epochs_without_improvement = 0\n",
    "  else:\n",
    "        epochs_without_improvement += 1\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"Early stopping en la época {epoch + 1}\")\n",
    "            break  # Salimos del bucle si no hay mejora\n",
    "\n",
    "print(\"Done!\")\n",
    "\n",
    "results6 = {\n",
    "    'ECM incorrectos sobre conjunto de validación': list_avg_train_loss_incorrecta_6,\n",
    "    'ECM sobre conjunto de entrenamiento n=256 p=0.1 bs=200': list_avg_train_loss_6,\n",
    "    'ECM sobre conjunto de validación n=256 p=0.1 bs=200': list_avg_valid_loss_6,\n",
    "    'model6_state_dict': model6.state_dict(),  # Guarda los pesos del modelo\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento Modelo 7: n = 256; p = 0.1; ADAM; LR = 1e-3; batch_size = 50\n",
    "\n",
    "n = 256\n",
    "p = 0.1\n",
    "autoencoder_conv_7 = Autoencoder_Conv(n=n, p=p)\n",
    "\n",
    "model7 = autoencoder_conv_7\n",
    "learning_rate = 1e-3\n",
    "\n",
    "optimizer = torch.optim.Adam(model7.parameters(), lr=learning_rate,eps=1e-08,weight_decay=0,amsgrad=False)\n",
    "\n",
    "batch_size = 50\n",
    "train_loader = DataLoader (train_set,batch_size=batch_size,shuffle=True)\n",
    "valid_loader = DataLoader (valid_set,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "list_avg_train_loss_incorrecta_7 = []\n",
    "list_avg_train_loss_7 = []\n",
    "list_avg_valid_loss_7 = []\n",
    "\n",
    "patience = 10  # Número de épocas sin mejora antes de detenerse\n",
    "min_delta = 0.001  # Mínima mejora en la pérdida de validación para considerarla una mejora\n",
    "\n",
    "best_valid_loss = float('inf')  # Inicializamos con un valor infinito\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "  avg_train_loss_incorrecta = train_loop(train_loader, model7, loss_fn, optimizer)\n",
    "  avg_train_loss = eval_loop(train_loader, model7, loss_fn)\n",
    "  avg_valid_loss = eval_loop(valid_loader, model7, loss_fn)\n",
    "  list_avg_train_loss_incorrecta_7.append(avg_train_loss_incorrecta)\n",
    "  list_avg_train_loss_7.append(avg_train_loss)\n",
    "  list_avg_valid_loss_7.append(avg_valid_loss)\n",
    "  print(\"Pérdida promedio incorrecta de entrenamiento:\",avg_train_loss_incorrecta)\n",
    "  print(\"Pérdida promedio de entrenamiento:\",avg_train_loss)\n",
    "  print(\"Pérdida promedio de validación:\",avg_valid_loss)\n",
    "\n",
    "  # Early stopping check \n",
    "  if avg_valid_loss < best_valid_loss - min_delta:\n",
    "        best_valid_loss = avg_valid_loss\n",
    "        epochs_without_improvement = 0\n",
    "  else:\n",
    "        epochs_without_improvement += 1\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"Early stopping en la época {epoch + 1}\")\n",
    "            break  # Salimos del bucle si no hay mejora\n",
    "\n",
    "print(\"Done!\")\n",
    "\n",
    "results7 = {\n",
    "    'ECM incorrectos sobre conjunto de validación': list_avg_train_loss_incorrecta_7,\n",
    "    'ECM sobre conjunto de entrenamiento n=256 p=0.1 bs=50': list_avg_train_loss_7,\n",
    "    'ECM sobre conjunto de validación n=256 p=0.1 bs=50': list_avg_valid_loss_7,\n",
    "    'model7_state_dict': model7.state_dict(),  # Guarda los pesos del modelo\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento Modelo 8: n = 256; p = 0.1; SGD; LR = 1e-3; batch_size = 50\n",
    "\n",
    "n = 256\n",
    "p = 0.1\n",
    "autoencoder_conv_8 = Autoencoder_Conv(n=n, p=p)\n",
    "\n",
    "model8 = autoencoder_conv_8\n",
    "learning_rate = 1e-3\n",
    "\n",
    "optimizer = torch.optim.SGD(model8.parameters(), lr=learning_rate)\n",
    "\n",
    "batch_size = 50\n",
    "train_loader = DataLoader (train_set,batch_size=batch_size,shuffle=True)\n",
    "valid_loader = DataLoader (valid_set,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "list_avg_train_loss_incorrecta_8 = []\n",
    "list_avg_train_loss_8 = []\n",
    "list_avg_valid_loss_8 = []\n",
    "\n",
    "patience = 10  # Número de épocas sin mejora antes de detenerse\n",
    "min_delta = 0.001  # Mínima mejora en la pérdida de validación para considerarla una mejora\n",
    "\n",
    "best_valid_loss = float('inf')  # Inicializamos con un valor infinito\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "  avg_train_loss_incorrecta = train_loop(train_loader, model8, loss_fn, optimizer)\n",
    "  avg_train_loss = eval_loop(train_loader, model8, loss_fn)\n",
    "  avg_valid_loss = eval_loop(valid_loader, model8, loss_fn)\n",
    "  list_avg_train_loss_incorrecta_8.append(avg_train_loss_incorrecta)\n",
    "  list_avg_train_loss_8.append(avg_train_loss)\n",
    "  list_avg_valid_loss_8.append(avg_valid_loss)\n",
    "  print(\"Pérdida promedio incorrecta de entrenamiento:\",avg_train_loss_incorrecta)\n",
    "  print(\"Pérdida promedio de entrenamiento:\",avg_train_loss)\n",
    "  print(\"Pérdida promedio de validación:\",avg_valid_loss)\n",
    "\n",
    "  # Early stopping check \n",
    "  if avg_valid_loss < best_valid_loss - min_delta:\n",
    "        best_valid_loss = avg_valid_loss\n",
    "        epochs_without_improvement = 0\n",
    "  else:\n",
    "        epochs_without_improvement += 1\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"Early stopping en la época {epoch + 1}\")\n",
    "            break  # Salimos del bucle si no hay mejora\n",
    "\n",
    "print(\"Done!\")\n",
    "\n",
    "results8 = {\n",
    "    'ECM incorrectos sobre conjunto de validación': list_avg_train_loss_incorrecta_8,\n",
    "    'ECM sobre conjunto de entrenamiento n=256 p=0.1 SGD': list_avg_train_loss_8,\n",
    "    'ECM sobre conjunto de validación n=256 p=0.1 SGD': list_avg_valid_loss_8,\n",
    "    'model8_state_dict': model8.state_dict(),  # Guarda los pesos del modelo\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento Modelo 9: n = 256; p = 0.1; convolución diferente; ADAM; LR = 1e-3; batch_size = 50; \n",
    "\n",
    "n = 256\n",
    "p = 0.1\n",
    "\n",
    "model_conv2 = AutoencoderConv2(n=n, p=p).to(device)\n",
    "\n",
    "learning_rate = 1e-3\n",
    "\n",
    "optimizer = torch.optim.Adam(model_conv2.parameters(), lr=learning_rate,eps=1e-08,weight_decay=0,amsgrad=False)\n",
    "\n",
    "batch_size = 50\n",
    "train_loader = DataLoader (train_set,batch_size=batch_size,shuffle=True)\n",
    "valid_loader = DataLoader (valid_set,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "list_avg_train_loss_incorrecta_conv2 = []\n",
    "list_avg_train_loss_conv2 = []\n",
    "list_avg_valid_loss_conv2 = []\n",
    "\n",
    "patience = 10  # Número de épocas sin mejora antes de detenerse\n",
    "min_delta = 0.001  # Mínima mejora en la pérdida de validación para considerarla una mejora\n",
    "\n",
    "best_valid_loss = float('inf')  # Inicializamos con un valor infinito\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "  avg_train_loss_incorrecta = train_loop(train_loader, model_conv2, loss_fn, optimizer)\n",
    "  avg_train_loss = eval_loop(train_loader, model_conv2, loss_fn)\n",
    "  avg_valid_loss = eval_loop(valid_loader, model_conv2, loss_fn)\n",
    "  list_avg_train_loss_incorrecta_conv2.append(avg_train_loss_incorrecta)\n",
    "  list_avg_train_loss_conv2.append(avg_train_loss)\n",
    "  list_avg_valid_loss_conv2.append(avg_valid_loss)\n",
    "  print(\"Pérdida promedio incorrecta de entrenamiento:\",avg_train_loss_incorrecta)\n",
    "  print(\"Pérdida promedio de entrenamiento:\",avg_train_loss)\n",
    "  print(\"Pérdida promedio de validación:\",avg_valid_loss)\n",
    "\n",
    "  # Early stopping check \n",
    "  if avg_valid_loss < best_valid_loss - min_delta:\n",
    "        best_valid_loss = avg_valid_loss\n",
    "        epochs_without_improvement = 0\n",
    "  else:\n",
    "        epochs_without_improvement += 1\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"Early stopping en la época {epoch + 1}\")\n",
    "            break  # Salimos del bucle si no hay mejora\n",
    "\n",
    "print(\"Done!\")\n",
    "\n",
    "results_conv2 = {\n",
    "    'ECM incorrectos sobre conjunto de validación': list_avg_train_loss_incorrecta_conv2,\n",
    "    'ECM sobre conjunto de entrenamiento conv2': list_avg_train_loss_conv2,\n",
    "    'ECM sobre conjunto de validación con2': list_avg_valid_loss_conv2,\n",
    "    'model_state_dict': model_conv2.state_dict(),  # Guarda los pesos del modelo\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 515
    },
    "executionInfo": {
     "elapsed": 1141,
     "status": "error",
     "timestamp": 1736647948825,
     "user": {
      "displayName": "MATÍAS GINO PRANZONI",
      "userId": "09458630644847038155"
     },
     "user_tz": 180
    },
    "id": "hjXNUUR6Uj8a",
    "outputId": "221cb791-0cca-4634-ead3-a63bf269908c"
   },
   "outputs": [],
   "source": [
    "# Ruta al archivo donde se guardaron los resultados\n",
    "filename1 = 'linealn256p02.pth'\n",
    "filename2 = 'linealn128p02.pth'\n",
    "filename3 = 'linealn64p02.pth'\n",
    "filename4 = 'linealn256p01.pth'\n",
    "filename5 = 'linealn256p01lr1e-2.pth'\n",
    "filename6 = 'linealn256p01bs200.pth'\n",
    "filename7 = 'linealn256p01bs50.pth'\n",
    "filename8 = 'linealn256p01SGD.pth'\n",
    "filename9 = 'conv2.pth'\n",
    "\n",
    "\n",
    "\n",
    "filepath1 = os.path.join('/Users/augusto/Documents/Python/Redes Neuronales', filename1)\n",
    "filepath2 = os.path.join('/Users/augusto/Documents/Python/Redes Neuronales', filename2)\n",
    "filepath3 = os.path.join('/Users/augusto/Documents/Python/Redes Neuronales', filename3)\n",
    "filepath4 = os.path.join('/Users/augusto/Documents/Python/Redes Neuronales', filename4)\n",
    "filepath5 = os.path.join('/Users/augusto/Documents/Python/Redes Neuronales', filename5)\n",
    "filepath6 = os.path.join('/Users/augusto/Documents/Python/Redes Neuronales', filename6)\n",
    "filepath7 = os.path.join('/Users/augusto/Documents/Python/Redes Neuronales', filename7)\n",
    "filepath8 = os.path.join('/Users/augusto/Documents/Python/Redes Neuronales', filename8)\n",
    "filepath9 = os.path.join('/Users/augusto/Documents/Python/Redes Neuronales', filename9)\n",
    "\n",
    "\n",
    "#torch.save(results1, filepath1)\n",
    "#torch.save(results2, filepath2)\n",
    "#torch.save(results3, filepath3)\n",
    "#torch.save(results4, filepath4)\n",
    "#torch.save(results5, filepath5)\n",
    "#torch.save(results6, filepath6)\n",
    "#torch.save(results7, filepath7)\n",
    "#torch.save(results8, filepath8)\n",
    "#torch.save(results_conv2, filepath9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificación de que Autoencoder efectivamente aprendió, con mejor Modelo encontrado\n",
    "figure = plt.figure()\n",
    "cols,rows = 2,3\n",
    "i = 0 #subplot index\n",
    "for row in range(1,rows+1):\n",
    "    j = torch.randint(len(train_set),size=(1,)).item() # Los números aleatorios tambien se pueden generar desde pytorch. Util para trabajar en la GPU.\n",
    "    #ploteamos la imagen original\n",
    "    i += 1\n",
    "    image,_ = train_set[j]\n",
    "    figure.add_subplot(rows,cols,i)\n",
    "    if row==1:\n",
    "       plt.title(\"Original\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(unbatch(image),cmap=\"Greys_r\")\n",
    "    #ploteamos la imagen predicha\n",
    "    i += 1\n",
    "    figure.add_subplot(rows,cols,i)\n",
    "    plt.axis(\"off\")\n",
    "    if row ==1:\n",
    "        plt.title(\"Predicha\")\n",
    "    image_pred = unbatch(model_conv2(batch(image)))\n",
    "    plt.imshow(image_pred,cmap=\"Greys_r\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "1qV4ZdROtu-eU_FnRTsRfrmquGAfX8UUy",
     "timestamp": 1736444430640
    },
    {
     "file_id": "1sjWEHZ1F5xN2cGuNme2cG0hD64YrEiG5",
     "timestamp": 1736437780463
    },
    {
     "file_id": "11i9OWd_kPdEiEyWdwL__9m09UM7jCdy7",
     "timestamp": 1736004135756
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
